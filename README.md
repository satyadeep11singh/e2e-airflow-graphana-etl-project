# E2E Insurance Data Pipeline

A complete end-to-end (E2E) data pipeline built with Apache Airflow on Astronomer Runtime. This project demonstrates a modern data lakehouse architecture with Landing â†’ Bronze â†’ Silver â†’ Gold layers.

## ğŸ¯ Project Overview

This project implements a three-stage data transformation pipeline for insurance data:

```
Landing Zone (CSV) 
    â†“
Bronze Layer (Parquet) [Raw/Immutable Data]
    â†“
Silver Layer (SQL Tables) [Cleaned/Standardized]
    â†“
Gold Layer (Fact Tables) [Business-Ready Analytics]
```

## ğŸ“Š Architecture

### **Data Layers**

| Layer | Technology | Purpose |
|---|---|---|
| **Landing** | CSV Files | Raw incoming data |
| **Bronze** | Parquet Files | Immutable raw copy with metadata |
| **Silver** | PostgreSQL Tables | Cleaned, deduplicated, standardized data |
| **Gold** | PostgreSQL Fact Tables | Enriched, joined, business-ready data |

### **Data Warehouse**

- **Database:** PostgreSQL 12.6
- **Host:** localhost
- **Port:** 5435
- **Credentials:** postgres / postgres
- **Schemas:** silver, gold

## ğŸš€ Getting Started

### Prerequisites

- Docker & Docker Compose
- Python 3.8+
- Astronomer CLI (optional)

### Start the Pipeline

```bash
cd c:\Users\satya\OneDrive\Desktop\e2e-project
astro dev start
```

This spins up:
- Airflow Scheduler (monitors DAGs)
- Airflow API Server (UI at http://localhost:8080)
- Airflow Triggerer (manages async tasks)
- DAG Processor (parses DAG files)
- PostgreSQL Database (port 5435)

### Access Airflow UI

```
URL: http://localhost:8080
Username: admin
Password: admin
```

## ğŸ“ Project Structure

```
e2e-project/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ 1_insurance_ingestion.py           # Landing â†’ Bronze
â”‚   â”œâ”€â”€ 2_bronze_to_silver.py              # Bronze â†’ Silver
â”‚   â”œâ”€â”€ 3_gold_transformation.py           # Silver â†’ Gold
â”‚   â””â”€â”€ .airflowignore
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ landing/                           # CSV input files
â”‚   â”‚   â””â”€â”€ archive/                       # Processed files
â”‚   â””â”€â”€ bronze/                            # Parquet files
â”œâ”€â”€ sql/                                   # SQL queries & tests
â”‚   â”œâ”€â”€ 01_reporting_queries.sql           # 66 reporting queries
â”‚   â”œâ”€â”€ 02_quality_tests.sql               # Data quality tests
â”‚   â””â”€â”€ 03_scheduling_and_monitoring.sql   # Monitoring queries
â”œâ”€â”€ docs/                                  # Documentation
â”œâ”€â”€ scripts/                               # Utility scripts
â”œâ”€â”€ config/                                # Configuration files
â”œâ”€â”€ plugins/                               # Custom operators
â”œâ”€â”€ tests/                                 # Test suites
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ packages.txt
â””â”€â”€ README.md
```

## ğŸ”„ DAGs (Data Pipelines)

### **DAG 1: `1_insurance_ingestion` (Landing â†’ Bronze)**

**Purpose:** Read CSV files, add metadata, convert to Parquet

**Schedule:** Daily at 2:00 AM UTC (0 2 * * *)

**Output:** Parquet files in `/include/bronze/` with `ingested_at` and `batch_id` columns

---

### **DAG 2: `2_bronze_to_silver` (Bronze â†’ Silver)**

**Purpose:** Load Parquet files into PostgreSQL with dynamic table creation

**Schedule:** Daily at 3:00 AM UTC (0 3 * * *) - 1 hour after DAG 1

**Features:**
- Dynamically creates separate tables for each parquet file
- Handles schema evolution (adds new columns as needed)

**Silver Tables Created (6 total):**
- `stg_premiums` (657 columns, 95,308 rows)
- `stg_acs_md_15_5yr_dp03_20251218` (556 columns)
- `stg_acs_md_15_5yr_dp05_20251218` (344 columns)
- `stg_territory_definitions_table_20251218` (8 columns)
- `stg_cgr_definitions_table_20251218` (10 columns)
- `stg_cgr_premiums_table_20251218` (14 columns)

---

### **DAG 3: `3_gold_transformation` (Silver â†’ Gold)**

**Purpose:** Create enriched fact table by joining all silver tables

**Schedule:** Daily at 4:00 AM UTC (0 4 * * *) - 1 hour after DAG 2

**Gold Table:**
- `fact_insurance_performance` (666 columns, 95,536 rows)

**Joins:** stg_premiums â†’ territory_definitions â†’ ACS_DP03 â†’ ACS_DP05

---

## ğŸ“ˆ Data Quality Metrics

| Metric | Value | Status |
|---|---|---|
| Total Records (Gold) | 95,536 | âœ… |
| Gender Completeness | 97.13% | âœ… |
| Batch Consistency | 100% | âœ… |
| Territory Matches | 1.20% | âš ï¸ Limited |
| Census Coverage | 1.22% | âš ï¸ Limited |

## ğŸ” Reporting & Analytics

SQL files located in `sql/` directory:

### **01_reporting_queries.sql**
66 comprehensive queries for:
- Basic reporting (summary stats, territory performance)
- Data quality (NULL checks, duplicate detection)
- Dashboard preparation (demographics, revenue analysis)
- KPIs (pipeline health, data quality scores)
- Data exports

### **02_quality_tests.sql**
Comprehensive data quality test suite:
- NULL checks (all 17 columns)
- Duplicate detection (exact & key-based)
- Data consistency validation
- Batch processing tests
- Automated alerting & monitoring
- Overall quality scorecards

### **03_scheduling_and_monitoring.sql**
DAG scheduling & monitoring:
- Cron expression reference
- Health check templates
- Weekly/monthly report queries
- Real-time monitoring
- PowerShell scheduling examples

### Quick Query Examples

```sql
-- Overall summary
SELECT COUNT(*) as total_records, 
       COUNT(DISTINCT territory_label) as unique_territories
FROM gold.fact_insurance_performance;

-- Territory performance
SELECT territory_label, COUNT(*) as record_count, 
       AVG(CAST(current_premium AS NUMERIC)) as avg_premium
FROM gold.fact_insurance_performance
WHERE territory_label != 'Unknown'
GROUP BY territory_label
ORDER BY record_count DESC;

-- Data quality check
SELECT 'Gender Completeness' as metric,
       ROUND(COUNT(CASE WHEN gender IS NOT NULL THEN 1 END)::NUMERIC 
       / COUNT(*)::NUMERIC * 100, 2)::TEXT || '%' as score
FROM gold.fact_insurance_performance;
```

## ğŸ—„ï¸ Database Connection

### Connection Details

```
Host: localhost
Port: 5435
Username: postgres
Password: postgres
Database: postgres
```

### Connect with psql

```bash
PGPASSWORD=postgres psql -h localhost -p 5435 -U postgres -d postgres
```

### Schemas

- **silver:** Staging tables (6 tables, 95k+ rows)
- **gold:** Fact tables (1 table, 95.5k rows)
- **public:** Default schema

## ğŸ”§ Common Tasks

### Run a DAG

1. Go to Airflow UI: http://localhost:8080
2. Find the DAG (e.g., `2_bronze_to_silver`)
3. Click "Trigger DAG" (play button)
4. Monitor in Logs tab

### Query the Database

```bash
# Connect
PGPASSWORD=postgres psql -h localhost -p 5435 -U postgres -d postgres

# List silver tables
\dt silver.*

# Query gold fact table
SELECT * FROM gold.fact_insurance_performance LIMIT 5;
```

### Run Data Quality Tests

```bash
PGPASSWORD=postgres psql -h localhost -p 5435 -U postgres -d postgres -f sql/02_quality_tests.sql
```

## ğŸš¨ Troubleshooting

### PostgreSQL Connection Issues

```bash
# Check if container is running
docker ps | grep postgres

# Test connection
PGPASSWORD=postgres psql -h localhost -p 5435 -U postgres -d postgres -c "SELECT version();"
```

### DAG Not Showing Up

- Check `dags/` for Python files
- Verify syntax: `python -m py_compile dags/*.py`
- Restart scheduler: `astro dev restart`

### Memory/Resource Issues

```bash
astro dev stop
astro dev start --no-cache
```

## ğŸ“š Resources

- [Airflow Documentation](https://airflow.apache.org/)
- [Astronomer Documentation](https://www.astronomer.io/docs/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [Parquet Format](https://parquet.apache.org/)

## âœ… Project Status

| Component | Status | Notes |
|---|---|---|
| Bronze Layer | âœ… Operational | CSV â†’ Parquet conversion |
| Silver Layer | âœ… Operational | Parquet â†’ SQL with schema evolution |
| Gold Layer | âœ… Operational | Enriched fact table with joins |
| Data Validation | âœ… Complete | Quality metrics established |
| Reporting Queries | âœ… Complete | 66 queries ready for BI tools |
| Data Quality Tests | âœ… Complete | Automated testing & monitoring |
| DAG Scheduling | âœ… Configured | Daily runs, staggered execution |
| Dashboards | ğŸ”„ Pending | Ready for BI tool integration |

---

**Last Updated:** December 19, 2025 | **Status:** âœ… Fully Operational | **File Structure:** âœ… Standardized
